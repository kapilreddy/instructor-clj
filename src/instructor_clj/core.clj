(ns instructor-clj.core
  (:require [cheshire.core :as cc]
            [clojure.string :as str]
            [litellm.core :as litellm]
            [malli.core :as m]
            [malli.json-schema :as json-schema]
            [stencil.core :as sc])
  (:import [com.fasterxml.jackson.core JsonParseException]))

(def ^:const default-client-params {:max-tokens 4096
                                    :temperature 0.7
                                    :model "gpt-3.5-turbo"})


(defn schema->system-prompt
  "Converts a malli schema into JSON schema and generates a system prompt for responses"
  [schema]
  (sc/render-string
   "As a genius expert, your task is to understand the content and provide
    the parsed objects in json that match the following json_schema:
    \n\n
    {{schema}}
    \n\n
    Make sure to return an instance of only the JSON.
    Refrain from returning the schema or any text explaining the JSON"
   {:schema (json-schema/transform schema)}))




(defn parse-generated-body
  "Parses the body of a response generated by an LLM API call.
   Extracts and converts the message content into a Clojure map."
  [body]
  (let [content (-> body
                    :choices
                    first
                    :message
                    :content)]
    (try
      (cc/parse-string content true)
      (catch JsonParseException _
        ;; Handle LLM returning JSON wrapped in markdown
        (when (re-find #"^```json" content)
          (-> content
              (str/replace #"^```json" "")
              (str/replace #"```$" "")
              str/trim
              (cc/parse-string true)))))))


(defn normalize-message
  "Normalizes a message map to ensure :role is a keyword.
   litellm-clj 0.3.0-alpha requires roles to be keywords."
  [message]
  (if (string? (:role message))
    (update message :role keyword)
    message))


(defn normalize-messages
  "Normalizes a collection of messages to ensure all roles are keywords."
  [messages]
  (mapv normalize-message messages))


(defn llm->response
  "The function performs the LLM call and tries to destructure and get the actual response.
   Returns nil in cases where the LLM is not able to generate the expected response.
   
   Supports multiple LLM providers through litellm-clj 0.3.0-alpha.
   Provider must be specified explicitly via :provider key (e.g., :openai, :anthropic, :gemini)."
  [{:keys [prompt response-schema max-tokens model temperature api-key provider]}]
  (let [messages [{:role :system
                   :content (schema->system-prompt response-schema)}
                  {:role :user
                   :content prompt}]
        ;; Use API key from environment if not provided
        api-key (or api-key (System/getenv "OPENAI_API_KEY"))
        ;; Build request map
        request-map {:messages messages
                     :temperature temperature
                     :max-tokens max-tokens}
        ;; Build config map
        config {:api-key api-key}
        ;; Call litellm with new 0.3.0-alpha API
        body (litellm/completion provider model request-map config)
        response (parse-generated-body body)]
    (when (m/validate response-schema response)
      response)))


(defn instruct
  "Attempts to obtain a valid response from the LLM based on the given prompt and schema,
   retrying up to `max-retries` times if necessary.
   
   Note: API keys can be provided via :api-key parameter or OPENAI_API_KEY environment variable."
  [prompt response-schema
   & {:keys [api-key _max-tokens _model _temperature max-retries] :as client-params
      :or {max-retries 0}}]
  (loop [retries-left max-retries]
    (let [params (merge default-client-params
                        client-params
                        {:prompt prompt
                         :response-schema response-schema
                         :api-key api-key})
          response (llm->response params)]
      (if (and (nil? response)
               (pos? retries-left))
        (recur (dec retries-left))
        response))))


(defn create-chat-completion
  "Creates a chat completion using litellm-clj (supports multiple LLM providers).

   Argument is a map with keys :messages, :model, :response-model, :provider, and optionally :api-key.
   :messages should be a vector of maps, each map representing a message with keys :role and :content.
   :model specifies the model to use (e.g., \"gpt-3.5-turbo\", \"claude-3-opus-20240229\", \"gemini-pro\").
   :response-model is a Malli schema specifying the expected response structure.
   :provider (required) - must be specified explicitly (e.g., :openai, :anthropic, :gemini, :mistral, :ollama)
   :api-key (optional) - if not provided, will use environment variable OPENAI_API_KEY

   Note: API keys can be set via environment variables:
   - OPENAI_API_KEY for OpenAI models
   - ANTHROPIC_API_KEY for Anthropic models
   - GEMINI_API_KEY for Google Gemini models
   - OPENROUTER_API_KEY for OpenRouter models

   Example:
   (require '[instructor-clj.core :as ic])

   (def User
     [:map
       [:name :string]
       [:age :int]])

   (ic/create-chat-completion
    {:messages [{:role \"user\", :content \"Jason Liu is 30 years old\"}]
     :model \"gpt-3.5-turbo\"
     :provider :openai
     :response-model User})

   Returns a map with extracted information in a structured format."
  ([client-params]
   (let [response-model (:response-model client-params)
         ;; Normalize messages to ensure roles are keywords (required by litellm-clj 0.3.0-alpha)
         user-messages (normalize-messages (:messages client-params))
         messages (apply conj
                         [{:role :system :content (schema->system-prompt response-model)}]
                         user-messages)
         model (:model client-params)
         ;; Provider must be explicitly provided
         provider (:provider client-params)
         ;; Use API key from environment if not provided
         api-key (or (:api-key client-params) (System/getenv "OPENAI_API_KEY"))
         ;; Build request map from default params and client params
         request-map (-> default-client-params
                         (merge (select-keys client-params [:max-tokens :temperature]))
                         (assoc :messages messages)
                         (dissoc :model))
         ;; Build config map
         config {:api-key api-key}
         ;; Call litellm with new 0.3.0-alpha API
         body (litellm/completion provider model request-map config)
         response (parse-generated-body body)]
     (if (m/validate response-model response)
       response
       body))))


;; Example usage
(comment

  ;; Set environment variable: export OPENAI_API_KEY=your-api-key
  
  (def User
    [:map
     [:name :string]
     [:age :int]])

  ;; Using the instruct function (simplified API)
  (instruct "John Doe is 30 years old."
            User
            :provider :openai
            :max-retries 0)

  (def Meeting
    [:map
     [:action [:and {:description "What action is needed"}
               [:enum "call" "followup"]]]
     [:person [:and {:description "Person involved in the action"}
               [:string]]]
     [:time [:and {:description "Time of the day"}
             [:string]]]
     [:day [:and {:description "Day of the week"}
            [:string]]]])

  ;; Using OpenAI model
  (instruct "Call Kapil on Saturday at 12pm"
            Meeting
            :provider :openai
            :model "gpt-4"
            :max-retries 2
            :api-key (System/getenv "OPENAI_API_KEY"))
  ;; => {:action "call", :person "Kapil", :time "12pm", :day "Saturday"}

  ;; Using create-chat-completion (more explicit)
  (create-chat-completion
   {:messages [{:role "user" :content "Call Kapil on Saturday at 12pm"}]
    :model "gpt-3.5-turbo"
    :provider :openai
    :response-model Meeting})

  ;; Using Anthropic Claude
  ;; Set environment variable: export ANTHROPIC_API_KEY=your-api-key
  (create-chat-completion
   {:messages [{:role "user" :content "Jason Liu is 30 years old"}]
    :model "claude-3-opus-20240229"
    :provider :anthropic
    :response-model User})

  ;; Using Google Gemini
  ;; Set environment variable: export GEMINI_API_KEY=your-api-key
  (create-chat-completion
   {:messages [{:role "user" :content "Jason Liu is 30 years old"}]
    :model "gemini-pro"
    :provider :gemini
    :response-model User})
  )
